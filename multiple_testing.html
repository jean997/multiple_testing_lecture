<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multiple Testing</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jean Morrison" />
    <script src="libs/header-attrs-2.11/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/robot-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/robot.css" rel="stylesheet" />
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <script src="libs/mark.js-8.11.1/mark.min.js"></script>
    <link href="libs/xaringanExtra-search-0.0.1/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search-0.0.1/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":true}) })</script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="scroll.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Multiple Testing
### Jean Morrison
### Adapted from Tom Braun
### March 29, 2022 (updated: 2022-03-29)

---


&lt;style&gt;

.remark-slide-content {
    font-size: 26px;
    padding: 1em 4em 1em 4em;
}

.remark-slide-content h1 {
  font-size: 3.2rem
}
&lt;/style&gt;


# Introduction

- Advancements in computing and experimental technology in the last ~60 years have allowed new types of experiments. 

- Technologies like microarrays and then later high throughput sequencing allow us to measure thousands of variables at once  (e.g. genes, genetic variants, etc). 

- We now want to perform thousands/millions/billions(?) of simultaneous hypothesis tests.

- This requires different concepts and techniques than individual hypothesis testings. 

- Resource for this lecture:

[Computer Age Statistical Inference by Bradley Effron and Trevor Hastie](https://hastie.su.domains/CASI/)




---
# Hypothesis Testing

- Let `\(X_1, \dots, X_N\)` be a sample of `\(N\)` observations from a probability distribution `\(\mathbb{P}\)`. 

- Let `\(\mathcal{M}\)` be a *model*.

  + A model is a set of probability distributions.
  
  + For example, we might have `\(\mathcal{M} = \lbrace N(\mu, \sigma^2); \mu \in \mathbb{R}, \sigma \in \mathbb{R}_{+} \rbrace\)`.
  
- A null hypothesis `\(H_0\)` is a subset of the model. 

  + Example: `\(H_0 = \lbrace N(0, \sigma^2); \sigma \in \mathbb{R}_{+}\rbrace\)`
  
- Almost always, the null hypothesis is defined by restricting a parameter, like the mean, to a subset.  

  + In fact almost always, our null is something like `\(H_0: \mu = 0\)` or `\(H_0: \mu \leq 0\)`. 
  
---
# Hypothesis Testing  

- Typically we try to answer statistical questions by identifying and estimating relevant parameters of the underlying probability distribution. 

  + E.g. the mean.
  + Or the conditional mean of one variable given another.  
  
- We might also interested in ruling out some (boring) values of the parameter space. 

  + E.g. mean is equal to 0 or mean is less than or equal to zero. 
  
- In hypothesis testing, we develop a test statistic `\(T(\mathbf{X})\)` such that `$$E_{\mathbb{P}}[T(\mathbf{X})] &gt; \underset{\mathbb{P}_0 \in H_0}{\text{sup}}E_{\mathbb{P}_0}[T(\mathbf{X})]$$` if `\(\mathbb{P} \not\in H_{0}\)`. 

---
# p-Values

- We want to know if the true data distribution `\(\mathbb{P}\)` is in `\(H_0\)` or not. 

--

**p-value**: The probability that the null hypothesis is true.

---
# p-Values

- We want to know if the true data distribution `\(\mathbb{P}\)` is in `\(H_0\)` or not. 

~~**p-value**: The probability that the null hypothesis is true.~~

Oh wait.  That's not right.

---
# p-Values

- We want to know if the true data distribution `\(\mathbb{P}\)` is in `\(H_0\)` or not. 

~~**p-value**: The probability that the null hypothesis is true.~~

**p-value**: The probability of observing a test statistic as or more extreme than actually observed, if `\(H_0\)` is true. 

`$$p(T(\mathbf{X}_{obs})) = \underset{\mathbb{P}_0 \in H_0}{\text{sup}} P_{\mathbb{P}_0}[T(\mathbf{X}) &gt; T(\mathbf{X}_{obs})]$$`

--

- Both the test statistic `\(T(\mathbf{X}_{obs})\)` and the p-value `\(p(T(\mathbf{X}_{obs}))\)` are random variables. 


---
# Distribution of p-values
 
- Suppose we have a sample of `\(N(\mu, \sigma^2)\)` data and we want to test the null hypothesis that `\(\mu \leq 0\)`. 

- We run a simple simulation: In each experiment, we

  +  Draw a sample of size 4 from a `\(N(\mu, 1)\)` distribution. 
  
  + Compute the t-statistic `\(T = \bar{X}/(\hat{\sigma}/\sqrt{4})\)`. 
  
  + Compute the p-value by computing the probability that a `\(t_3\)` distributed variable is larger than `\(T\)`. 
  
- Repeat 10,000 times

Murdoch, Tsai, and Adcock (2008). P-Values are Random Variables. *The American Statistician*, 62, 242-245.
---
# Distribution of p-values

- If `\(\mu = 0\)` (the boundary of `\(H_0\)`) the p-values are exactly uniformly distributed. 

&lt;center&gt; 
&lt;img src="img/uniform_p.png" width="40%" /&gt;
&lt;/center&gt;

---
# Distribution of p-values

- Under the alternative, the p-value is not uniform. The distribution will be shifted towards 0. 

&lt;center&gt; 
&lt;img src="img/alt_pval.png" width="85%" /&gt;
&lt;/center&gt;

---
# Distribution of p-values

- If the true value `\(\mu\)` is -0.5 (inside the null region), the distribution of p-values will be shifted towards 1.  

&lt;center&gt; 
&lt;img src="img/deep_pval.png" width="40%" /&gt;
&lt;/center&gt;

---
# Distribution of p-values

- The distribution of the p-value depends on the true value of the parameter. 

--

- The p-value is not guaranteed to be small under the alternative hypothesis. 

  + When `\(\mu = 0.5\)`, the p-value is less than 0.05 in about 20% of simulations. 
  
  + Recall power from last week's lecture. 
  
--

- This is why we can't accept the null hypothesis: 

  + "We conclude that there is no effect of TRIP13 on tumor growth" vs
  
  + "We are not able to reject the null hypothesis that TRIP13 has no effect on tumor growth."
  
  
---
# Decision Rules

- The act of rejecting or not rejecting a null hypothesis is a decision.

- Our decision will either be correct or incorrect depending on the underlying (unknowable) truth.

- Because we are making a binary decision and the underlying truth is binary, our decision will fall into one of four categories.

- If we use the decision rule `\(p &lt; 0.05\)`, then 0.05 is a *critical value*. 

&lt;/br&gt;

&lt;table class="table table-bordered" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Don't Reject \(H_{0}\) &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Reject \(H_{0}\) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; \(H_{0}\) True &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; True Negative &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; False Positive &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; \(H_{0}\) False &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; False Negative &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; False Positive &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
# Multiple Testing

- Suppose that instead of testing one null hypothesis, we are now testing a large number of null hypotheses in a single experiment. 

  + For example, we might be interested in the difference in average gene expression between two groups across many genes...
  
  + Let `\(m\)` be the total number of hypotheses tested.
  
--

- Suppose that the global hull is true: `\(H_0\)` is true for every hypothesis tested. 

- If we reject `\(H_0\)` for all parameters with `\(p &lt; 0.05\)`, we can expect to make `\(0.05\cdot m\)` mistakes. 
  + If `\(m = 20,000\)`, this is 1,000 mistakes. 
  
  + This could lead to a lot of wasted money if we follow up our discoveries with expensive experiments. 

---
# Family-Wise Error Rate

- The family-wise error rate is the probability of making *any* mistakes among a set of tests under the global null. 

- The *Bonferoni correction* is an adjustment to the critical value for our decision rule. 

- Suppose we want to control the family-wise error rate at a level `\(\alpha\)`. 

- The Bonferroni correction tells us to reject when `\(p &lt; \alpha/m\)`. 

---
# Bonferroni Correction

Under the global null,

`$$P(\text{any } p_j &lt; \alpha/m) = P(\bigcup_{j=1}^m\lbrace p_j \leq \alpha/m \rbrace)\\\
\leq \sum_{j=1}^m P(p_j \leq \alpha/m)\\\
= \sum_{j = 1}^m \alpha/m = \alpha$$`

- This bound holds for any dependence structure between the `\(p_j\)`.


---
# Bonferroni and Dependence

- Suppose that our `\(p\)`-values are highly correlated. 

- In the worst case, our tests are nearly identical. 

- In this case, Bonferroni will be very conservative. 

- Our actual FWER will be
`$$P(\text{any } p_j &lt; \alpha/m) \approx P(p_1 &lt; \alpha/m) \approx \alpha/m$$`

- We would have been better off rejecting when `\(p_j &lt; \alpha\)`.

---
# Bonferroni and Dependence


- Bonferroni is more conservative when `\(m\)` is larger and when correlation between `\(p\)`-values is greater. 

&lt;center&gt; 
&lt;img src="img/bonferroni.png" width="60%" /&gt;
&lt;/center&gt;

---
# Sequential Holm-Bonferroni Method

- We can do a little better than Bonferroni. 

- Order `\(p\)`-values from smallest to largest.

`$$p_{(1)} \leq p_{(2)} \leq \dots \leq p_{(i)} \leq \dots p_{(N)}$$`
- Let `\(i_0\)` be the smallest index `\(i\)` such that
`$$p_{(i_0)} &gt; \frac{\alpha}{N-i_0 + 1}$$`

- Reject all null hypothesis for `\(H_{0(i)}\)` for `\(i &lt; i_0\)`. 

---
# Holm-Bonferroni Example

- Suppose we have 5 p-values: `\(p_1 = 0.015\)`, `\(p_2 = 0.032\)`, `\(p_3 = 0.002\)`, `\(p_4 = 0.046\)`, `\(p_5 = 0.005\)`.

- Order them: 0.002, 0.005, 0.015, 0.032, 0.046.

- `\(0.002 &lt; \frac{0.05}{5-1+1} = 0.01\)`: Reject `\(H_3\)`. 

- `\(0.005 &lt; \frac{0.05}{5-2 + 1}= 0.0125\)`: Reject `\(H_5\)`.

- `\(0.015 &lt; \frac{0.05}{5-3+1} = 0.017\)`: Reject `\(H_1\)`.

- `\(0.032 &gt; \frac{0.05}{5-4+1} = 0.025\)`: Stop. Fail to reject `\(H_2\)` and `\(H_4\)`. 

- Bonferroni would only allow us to reject `\(H_3\)` and `\(H_5\)`. 





---
# Why FWER?

- Controlling the family-wise error rate is about behavior under the **global null**. 

- Often when we are doing many tests, we know confidently that the global null does not hold. 

- For example, in genetic studies we *know* there are some genetic effects on a trait because we know the trait runs in families. 

- We *know* that gene expression differs for some genes between cell types because different cell types do different things. 

- Our interest is in determining which hypotheses are non-null, not whether any hypotheses are non-null.  

---
# Microarray Example

- Gene expression for 6,033 genes was measured by microarray for 52 prostate cancer patients and 50 controls. 

- For each gene, a two sample `\(t\)` statistic was computed. 

- `\(t\)`-statistics were converted to `\(z\)`-values by inverting the CDF of the the `\(t\)`-distribution. 

&lt;center&gt; 
&lt;img src="img/prostate_hist.png" width="55%" /&gt;
&lt;/center&gt;

---
# False Discovery Rate

- We want a decision rule to reject or not reject `\(H_0\)` for each gene.

- Our collaborators are going to perform a lab experiment to further study each of the genes for which we rejected `\(H_0\)`. 

- Experiments are expensive and time-consuming so we want to limit the number of false positives in our rejected set. 

- We want to tell our collaborators what proportion of the experiments we think will be a waste of time. 

- We need to know the *false discovery rate* or the expected proportion of rejections for which `\(H_0\)` was actually true.

---
# False Discovery Rate

- Suppose we apply a decision rule to a set of `\(N\)` hypotheses, producing the table below: 
&lt;center&gt; 
&lt;img src="img/twobytwo.png" width="40%" /&gt;
&lt;/center&gt;

- The *false discovery proportion (Fdp)* is `\(a/R\)`. Fdp is defined to be 0 if `\(R = 0\)`.

- FDP is unobservable, but we may be able to control `\(E[Fdp]\)`. 

---
# False Discovery Rate

- The false discovery rate of a decision rule `\(D\)` is the expectation of its Fdp. 
`$$FDR(D) = E_P(Fdp(D))$$`
where `\(P\)` is the distribution of `\(p\)`-values. 

- We want to produce a rule such that `\(FDR(D) &lt; q\)` for some `\(q \in (0, 1)\)`. 

---
# Benjamini Hochberg FDR

Define a decision rule `\(D_q\)`:

- Order `\(p\)`-values smallest to largest:
`$$p_{(1)} \leq p_{(2)} \leq \dots \leq p_{(i)} \leq \dots p_{(N)}$$`

- Find `\(i^*\)`, the largest index `\(i\)` such that 
`$$p_{(i^*)} \leq q \frac{i^*}{N}$$`

- Reject null hypotheses if `\(H_{0(i)} \leq i^*\)`. 

- `\(D_q\)` has FDR equal to `\(\pi_0 q \leq q\)` where `\(\pi_0 = \frac{N_0}{N}\)`, the proportion of truly null hypotheses. 

---
# BH vs Holm-Bonferroni

Black line: Rejection threshold for Holm-Bonferroni at `\(\alpha = 0.1\)`.

Red line: Rejection threshold for Benjamini-Hochberg at `\(q = 0.1\)`. 

&lt;center&gt; 
&lt;img src="img/fdr_fwer.png" width="75%" /&gt;
&lt;/center&gt;

---
# Multiple Testing Adjustment in R

- The function `p.adjust` in the `stats` package implements the Bonferroni, Holm-Bonferroni, Benjamini-Hochberg, and several other multiple testing adjustments. 

- The input to `p.adjust` is a vector of `\(p\)`-values, `\(p_1, \dots, p_N\)`.

- The output is a vector critical values `\(a_1, \dots, a_N\)`. 

- For BH, `\(a_j\)` is the largest FDR threshold at which we would reject `\(H_{0}\)` for all hypotheses with `\(p\)`-value less than or equal to `\(p_j\)` but would not reject hypotheses.

  - For Bonferroni and Holm-Bonferroni swap FWER for FDR in the statement above.
  - For FDR procedures, these are called `\(q\)`-values. 
  
- So a decision rule controlling the FDR at `\(q &lt; 0.1\)` would reject all tests with `p.adjust(pvals, method = "BH") &lt; 0.1`. 

---
# Benjamini-Hochberg and Dependence

- The BH procedure is guaranteed to control the FDR if tests are independent. 

- This guarantee extends to some situations with dependence. 
  
  + In particular, if all correlations between test-statistics are positive, BH remains valid. 
  
  
- For general dependence structures, an alternative method Benjamini-Yekutieli (BY) can be used. 

- BY tends to be much more conservative than BH.

---
# FDR Considerations

- Controlling the FDR only controls the *average* false discovery proportion. 

- We don't get any guarantees about the probability that the Fdp is bigger than `\(q\)`. 

- We also don't get guarantees that the Fdp is smaller than some threshold. 

  + If the global null is true, the Fdp is 1 if we make any rejections. 
  + The BH procedure therefore controls FDR by making no rejections more than 1-q% of the time. 
  + But in the cases where it does make rejections, all discoveries are false. 

- Whether or not a hypothesis is rejected will depend on the distribution of the rest of the test statistics. 
  
  + If we have lots of confident rejections, we can afford more iffy rejections. 


---
# Controlling FDR

The code below runs a very simpl simulation. 

- Generate 1100 z-scores. The first 100 are drawn from a `\(N(0,1.5^2)\)` distribution (non-null), the remaining 1000 are drawn from a `\(N(0,1)\)` distribution. 

- Compute a `\(p\)`-value comparing the z-scores to the standard normal distribution. 

- Compute critical values using the BH method. 

- Compute the Fdp


```r
fdp &lt;- replicate(n  = 1000, expr ={
  z &lt;- c(rnorm(n = 100, sd = 1.5), rnorm(n = 1000))
  p &lt;- 2*pnorm(-abs(z))
  x &lt;- which(p.adjust(p, method = "BH") &lt; 0.1)
  if(length(x)==0) r &lt;- 0
      else r &lt;- sum(!x %in% 1:100)/length(x)
  c(r, length(x))
})
hist(fdp[1,], breaks = seq(0, 1, length.out = 50), 
     xlab = "Fdp", main = "")
```

&lt;img src="multiple_testing_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---
# Controlling FDR

- In our simulation, the average Fdp was 0.09. 

- We made between 0 and 14 rejections with an average of 1.848 rejections per experiment.  

- We made 0 rejections in 34% of the experiments. 

- The Fdp was greater than 0.2 in 18% of experiments and greater than 0.5 in 4% of experiments.

- The Fdp was 0 in 78% of experiments.

---
# A Bayesian View

- Our problem is a classification problem: we want to sort hypotheses into categories " `\(H_0\)` true" and " `\(H_0\)` false". 

- Using Bayesian methods, we can make statements about the probability that `\(H_0\)` is false or not.

- Suppose we have a set of test statistics `\(\hat{z}_1, \dots, \hat{z}_m\)`. 

- The Bayesian false discovery rate of a values `\(z\)` is 

`$$Fdr(z) = P(H_{0,j} \text{ true} \vert \hat{z}_j &gt; z)$$`
---
# Bayesian False Discovery Rate

- We assume the data are from a mixture distribution. 

  + Test statistics for null tests have density `\(f_0(z)\)`.
  + Test statistics for non-null tests have density `\(f_1(z)\)`. 
  
- Typically, we assume we know `\(f_0(z)\)` (e.g `\(N(0,1)\)`). 

- If we knew `\(f_1(z)\)`, we could compute the Bayesian Fdr using Bayes rule. 

`$$Fdr(z) = P(H_{0,j} \text{ true} \vert \hat{z}_j &gt; z) = \\\
      \frac{P(H_{0,j} \text{ true})P(\hat{z}_j &gt; z \vert H_{0,j} \text{ true})}{P(\hat{z}_j &gt; z)} = \\\
      \frac{\pi_0 \int_{z}^\infty f_0(z)dz}{\int_z^\infty f(z)dz}$$`
    where 
    `$$f(z) = \pi_0 f_0(z) + \pi_1 f_1(z)$$`
    and `\(\pi_0\)` and `\(\pi_1\)` are the prior probability of `\(H_0\)` and `\(H_a\)` respectively. 


---
# Estimating the Bayesian Fdr

`$$Fdr(z) = \frac{\pi_0 \int_{z}^\infty f_0(z)dz}{\int_z^\infty f(z)dz} = \frac{\pi_0 S_0(z)}{S(z)}$$`
- We know `\(f_0\)` and (typically) can assume `\(\pi_0\)` is close to 1. 

- This leaves us to estimate the denominator which is just the total probability of observing a test statistic greater than `\(z\)`. 

- We can use the empirical estimate

`$$\hat{S}(z) = \frac{1}{m}\sum_{j=1}^m 1_{\hat{z}_j &gt; z}$$`

- This procedure is "empirical Bayes" because we are estimating `\(S(z)\)` from the data. 


---
# Bayesian false discovery rate - BH Connection

- First notice that the p-value `\(p_j = = \int_{z_j}^\infty f_0(z)dz = S_0(z_j)\)`. 

- Now order the p-values as we did for the BH method. 

- Notice that the empirical estimate of `\(\hat{S}(z_{(i)}) = \frac{i}{m}\)`

- So 
`$$Fdr(z_{(i)}) = \frac{\pi_0 p_{(i)}}{i/m}$$`
- Thus, if we wanted to control the Bayesian Fdr at a threshold `\(q\)`, we would reject `\(H_0(i)\)` if

`$$p_{(i)} &lt; q \frac{i}{m \pi_0}$$`

- If we set `\(\pi_0\)` to 1, we have the Benjamini-Hochberg procedure. 

- So in a frequentist view, the BH procedure controls the average FDR. 

- In a Bayesian view, the BH procedure controls the "posterior probability of nullness". 

---
# Bayesian FDR and Dependence

- The estimate of `\(\hat{S}\)` is unbiased regardless of the dependence structure of the data. 

- However, more correlation increases the variability of `\(\hat{S}\)` and the actual FDR. 

---
# Local False Discovery Rate

- The Bayesian FDR gives the probability of `\(H_{0,j}\)` given that `\(\hat{z}_j\)` is greater than some threshold `\(z\)`. 

- However, we may be more interested in the probability that `\(H_{0,j}\)` given `\(\hat{z}_j\)`. This is the local false discovery rate. 

`$$lfdr(z) = P(H_{0,j} \text{ true} \vert \hat{z}_j = z)$$`
- Using Bayes theorem, we find
`$$lfdr(z) = \frac{\pi_0 f_0(z)}{f(z)}$$`

- We can estimate `\(f(z)\)` by drawing a smooth estimate through the histogram of test-statistics. 

- Implemented in the `locfdr` R package. 

---
# Local False Discovery Rate in R



```r
library(locfdr)
prostz &lt;- read.csv("http://hastie.su.domains/CASI_files/DATA/prostz.txt")
prostz &lt;- prostz[,1]
zz &lt;- locfdr(prostz, nulltype = 0, plot = 1)
```

&lt;img src="multiple_testing_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

---
# lfdr vs Bayesian Fdr

&lt;center&gt; 
&lt;img src="img/lfdr_Fdr.png" width="65%" /&gt;
&lt;/center&gt;


- The local false discovery rate is more conservative than the Bayesian FDR. 

- In the prostate data, the Benjamini-Hochberg method rejects 60 hypotheses at a threshold of `\(q &lt;0.1\)`. Using the `locfdr` procedure, we reject only 25 hypotheses at `\(lfdr_j &lt; 0.1.\)`


---
# Empirical Bayes Alternatives

- The `locfdr` method is partitioning the distribution of test statistics into null and non-null portions

&lt;center&gt; 
&lt;img src="img/locfdr.png" width="65%" /&gt;
&lt;/center&gt;

- Different assumptions about `\(f_1\)` lead to different partitions. 

---
# Empirical Bayes Alternatives

&lt;center&gt; 
&lt;img src="img/qvalue.png" width="65%" /&gt;&lt;img src="img/ash.png" width="65%" /&gt;
&lt;/center&gt;

- Adaptive Shrinkage (ash) assumes that non-null effects are unimodal. 

  + Uses a flexible empirical estimate of non-null distribution. 
  
  + Implemented in the `ashr` R package.
  
  + Matthew Stephens (2017). False Discovery Rates: A New Deal. *Biostatistics*. doi:10.1093/biostatistics/kxw041

---
# Adaptive Shrinkage in R

- ASH can accept estimates that are expected to be mean 0 normally distributed under the null *and* standard errors ( `\(\hat{\beta}\)` and `\(s\)` ). 
  + `locfdr` requires z-values. 
  
- ASH will also compute the local false sign rate, or the posterior probability of getting the sign wrong. It is the smaller of 

`$$P[\beta_j &lt; 0 \vert \hat{\beta}, s, \hat{\pi}] \qquad \text{and} \qquad P[\beta_j &gt; 0 \vert \hat{\beta}, s, \hat{\pi}]$$`

  + `\(\hat{\pi}\)` are estimated parameters defining `\(f_1\)`. 


```r
library(ashr)
zzash &lt;- ash(betahat = prostz, sebetahat = 1)

plot(zz$fdr, zzash$result$lfdr, xlab = "locfdr lfdr", ylab = "ASH lfdr")
abline(0, 1)
```

![](multiple_testing_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;


- `locfdr` estimated that 93% of hypotheses in the prostate cancer data were null while ash estimates that 89% of hypotheses are null. 


---
# Summary

- The method we use to declare hypotheses "significant" depends on the context. 

- Bonferroni adjustment can be very conservative.

- If the global null is not interesting, we can do better by controlling the FDR. 

- Empirical Bayes methods give estimates of the Bayesian FDR and the local false discovery rate. 
  
    + In some contexts, more interpretable and useful than average FDR control. 
    


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"scroll": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
